{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d4db86",
   "metadata": {},
   "source": [
    "# Support Vector Machines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665e30f",
   "metadata": {},
   "source": [
    "![image](../images/svm.png)\n",
    "\n",
    "$\\text{Datatset = }(x_1, y_1),(x_2,y_2),...(x_N,y_N)$; \n",
    "\n",
    "$x_i\\in \\R^p$ ,  $y_i\\in \\{-1,1\\}.$ \n",
    "\n",
    "$\\text{Hyperplane: } x^T\\beta +\\beta_0 = 0$ , where $||\\beta||=1.$  \n",
    "\n",
    "$\\text{Classification Rule: }G(x) = sign[x^T\\beta +\\beta_0]$\n",
    "\n",
    "$\\text{Optimization Problem : }\\max\\limits_{{\\beta,\\beta_0, ||\\beta_0||=1}} M \\ni y_i(x_i^T\\beta + \\beta_0 ) \\geq M ,  i = 1,..., N$  \n",
    "\n",
    "$\\text{Margin: }M = \\frac{1}{||\\beta||}$  \n",
    "\n",
    "The optimization problem can be conveniently rephrased as follows; \n",
    "\n",
    "$\\min\\limits_{{\\beta,\\beta_0}} ||\\beta|| \\ni y_i(x_i^T\\beta + \\beta_0 ) \\geq 1 ,  i = 1,..., N$\n",
    "\n",
    "---\n",
    "\n",
    "Suppose now that the classes overlap in feature space(second figure)  → Allowing some points to be on the wrong side of the margin → still we need to maximize M.\n",
    "\n",
    "$\\text{Modification of Optimization:       }y_i (x_i^T\\beta +\\beta_0)\\geq M(1-\\xi)$\n",
    "\n",
    "---\n",
    "\n",
    "$\\xi _i$  in the constraint $y_i(x_i ^T\\beta +\\beta_0)\\geq M(1-\\xi_i)$  is the proportional amount by which the prediction $f(x_i) = x_i ^T\\beta + \\beta_0$  is on the wrong side of its margin. \n",
    "\n",
    "Hence by bounding the sum $\\sum{\\xi_i}$, we bound the total proportional amount by which predictions fall on the wrong side of their margin. \n",
    "\n",
    "Misclassifications occur when $\\xi_i \\gt1$, so the above bounding at a value $K$ , say , bounds the total number of training misclassifications at $K$. \n",
    "\n",
    "---\n",
    "\n",
    "### Optimization using Lagrange Multipliers\n",
    "\n",
    "$\\min\\limits{{\\beta,\\beta_0}}    \\frac{1}{2} ||\\beta||^2 + C\\sum{\\xi_i }$  \n",
    "\n",
    "subject to $\\xi_i \\ge 0 ,    y_i(x_i ^T\\beta + \\beta_0 ) \\gt 1- \\xi_i \\forall i$ \n",
    "\n",
    "→ $L_p = \\sum \\alpha_i - \\frac{1}{2} \\sum \\sum \\alpha_i \\alpha_j y_i y_j x_i ^T x_j$ \n",
    "\n",
    "→ $L_p = \\sum \\alpha_i - \\frac{1}{2} \\sum \\sum \\alpha_i \\alpha_j y_i y_j <\\phi(x_i),\\phi(x_j)>$  \n",
    "\n",
    "→ If we have a kernel function then we don’t need to compute transformation \n",
    "\n",
    "→ $K(x_i, x_j )  = <\\phi(x_i), \\phi(x_j)>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a75b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
